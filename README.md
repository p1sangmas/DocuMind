# ğŸ§  DocuMind - AI-Powered Knowledge Base Assistant

<div align="center">
  <img src="assets/logo.png" alt="DocuMind Logo" width="200">
  <br>
  <img src="https://img.shields.io/badge/version-1.1.0-green?style=flat-square" alt="Version">
  <img src="https://img.shields.io/badge/license-MIT-blue?style=flat-square" alt="License">
  <img src="https://img.shields.io/badge/platform-Docker-blue?style=flat-square" alt="Platform">
  <img src="https://img.shields.io/badge/LLM-Llama%203.2%203B-orange?style=flat-square" alt="LLM">
</div>

## ğŸš€ Overview

DocuMind is a privacy-focused, self-hosted AI assistant that helps you extract insights from your PDF documents using local LLMs through Ollama. Ask questions about your documents in natural language and receive accurate answers with source citations - all without sending your data to external services.

<div align="center">
  <h3>ğŸ“„ Upload Documents â†’ ğŸ” Ask Questions â†’ ğŸ¤– Get AI Answers</h3>
</div>

## âœ¨ Key Features

- **ğŸ”’ Privacy First**: All processing happens locally - no external API calls
- **ğŸ“„ Multi-format PDF Processing**: Robust text extraction with OCR support
- **ğŸ” Hybrid Retrieval System**: Combines semantic and keyword search for accuracy
- **ğŸ¤– Local LLM Integration**: Uses Ollama (Llama 3.2 3B) for responses
- **ğŸ’¬ Conversation Memory**: Maintains context across multiple questions
- **ğŸ“Š Source Attribution**: Shows which documents informed each answer
- **ğŸ”„ Automatic Document Loading**: Auto-loads PDFs from the documents directory
- **ğŸŒ Dual Interfaces**: Both Streamlit UI and HTML/CSS/JS web interface
- **âš¡ Docker Ready**: Simple setup with Docker and GPU acceleration support

## ğŸš€ Quick Start with Docker (Recommended)

The easiest way to get started is using the included Docker helper script:

```bash
# Make the script executable (if needed)
chmod +x run_docker.sh

# Run the script and follow the menu options
./run_docker.sh
```

Select option 1 from the menu to start DocuMind, then:
- **Web UI**: http://localhost:8080
- **API Endpoint**: http://localhost:8000/api

## ğŸ’» Manual Setup (Alternative)

If you prefer to run without Docker:

1. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Install Ollama**
   ```bash
   # Follow Ollama installation instructions from https://ollama.ai/
   # Run the Ollama service
   ollama serve
   ```

3. **Install OCR Dependencies (Optional)**
   ```bash
   pip install pytesseract pdf2image pillow
   brew install tesseract poppler  # For macOS
   # See documentation/OCR_SETUP.md for other OS instructions
   ```

3. **Add Documents**
   - Place PDF files in the `data/documents` directory

4. **Run the Application**
   ```bash
   # Start the Web Interface
   python api.py
   
   # OR start the Streamlit Interface
   streamlit run app.py
   ```

## ğŸ”§ System Architecture

```
DocuMind/
â”œâ”€â”€ app.py                     # Main Streamlit application
â”œâ”€â”€ api.py                     # Alternative web interface (HTML/CSS/JavaScript)
â”œâ”€â”€ docker-entrypoint.sh       # Docker container startup script
â”œâ”€â”€ docker-compose.yml         # Container orchestration configuration
â”œâ”€â”€ docker-compose.gpu.yml     # GPU support configuration
â”œâ”€â”€ Dockerfile                 # Container definition
â”œâ”€â”€ run_docker.sh              # Docker helper script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ document_processor.py  # PDF processing and extraction with OCR
â”‚   â”œâ”€â”€ chunking.py            # Semantic text chunking
â”‚   â”œâ”€â”€ retriever.py           # Hybrid retrieval system
â”‚   â”œâ”€â”€ llm_handler.py         # LLM integration and prompts
â”‚   â”œâ”€â”€ evaluator.py           # Evaluation framework
â”‚   â”œâ”€â”€ preload_models.py      # Model preloading script
â”‚   â””â”€â”€ utils.py               # Utility functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/             # PDF documents for auto-loading
â”‚   â”œâ”€â”€ vectorstore/           # Chroma vector database
â”‚   â”œâ”€â”€ models_cache/          # Hugging Face model cache
â”‚   â””â”€â”€ chroma_cache/          # ChromaDB ONNX model cache
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py            # Configuration settings
â”œâ”€â”€ documentation/             # Detailed documentation files
â”œâ”€â”€ tests/                     # Testing and diagnostic tools
â””â”€â”€ web/                       # Web UI assets (HTML/CSS/JS)
```

## ğŸ“Š Performance Optimization

### Embedding Model Caching

DocuMind pre-downloads and caches embedding models to improve startup and query time:

- Models are stored in `./data/models_cache/`
- ONNX optimized versions are kept in `./data/chroma_cache/onnx_models/`

### LLM Selection

Choose the right LLM based on your hardware:
- **High-end systems**: Use larger models like `llama3.2:3b` (default)
- **Low-resource systems**: Switch to `phi3:mini` for faster responses (option 5 in the run_docker.sh menu)

## ğŸ” Advanced Features

### Auto-Loading Documents
- Documents placed in the `data/documents` directory are automatically loaded when the app starts
- Configure auto-loading behavior in `config/settings.py`:
  ```python
  AUTO_LOAD_DOCUMENTS = True  # Enable/disable auto-loading
  AUTO_LOAD_SKIP_EXISTING = True  # Skip already processed documents
  ```

### OCR Support for Problematic PDFs
- OCR (Optical Character Recognition) processing for difficult PDFs
- Automatically detects when a PDF needs OCR and applies it
- Perfect for PDFs saved from websites that have selectable text but don't parse correctly
- See [OCR Setup Guide](documentation/OCR_SETUP.md) for detailed setup instructions

### PDF Diagnostic Tool
- Use `tests/check_pdf.py` to diagnose problematic PDFs:
  ```bash
  python tests/check_pdf.py path/to/document.pdf
  ```
- Identifies which extraction method works best for each document
- Determines if OCR processing is recommended

## ğŸ› ï¸ Troubleshooting Common Issues

### 1. Timeout Error During Query Processing

**Symptom**: Requests timeout with error: "Error generating response: Read timed out."

**Solution**:
- Switch to a smaller LLM model through option 5 in the run_docker.sh script
- Restart the containers to apply changes

### 2. Document Loading Issues

**Symptom**: Documents fail to load or extract properly

**Solution**:
- Check the format of your PDF
- Run diagnostic tool: `python tests/check_pdf.py path/to/document.pdf`
- Enable OCR for problematic documents

### 3. Ollama Connection Issues

**Symptom**: Error connecting to Ollama service

**Solution**:
- For Docker: Ensure the Ollama container is running (`docker ps`)
- For manual setup: Make sure Ollama is running (`ollama serve`)
- See [Environment Setup Guide](documentation/ENVIRONMENT_SETUP.md) for details on connection configuration

For more troubleshooting tips, see the [Full Documentation](documentation/DOCUMENTATION.md#troubleshooting).

## ğŸ“š Documentation

Comprehensive documentation is available in the `documentation` folder:

- [Complete User Guide](documentation/USER_GUIDE.md)
- [Docker Setup Guide](documentation/DOCKER.md)
- [Environment Setup Guide](documentation/ENVIRONMENT_SETUP.md)
- [OCR Setup Instructions](documentation/OCR_SETUP.md)
- [Full Technical Documentation](documentation/DOCUMENTATION.md)

## ğŸ“‹ Technology Stack

- **Document Processing**: PyPDF2, PyMuPDF, pdfplumber, Tesseract OCR
- **Embeddings**: Sentence-Transformers (all-MiniLM-L6-v2)
- **Vector Database**: ChromaDB
- **LLM**: Ollama (Llama 3.2 3B)
- **Frontend**: Streamlit, HTML/CSS/JavaScript
- **Backend**: Python FastAPI
- **Containers**: Docker, Docker Compose

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Developed with â¤ï¸ by Fakhrul Fauzi.
